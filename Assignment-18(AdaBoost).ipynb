{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b85e09",
   "metadata": {},
   "source": [
    "### 1. What is Ensemble Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1270b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble learning is a learning technique in which multiple individual models combine to create\n",
    "a master model.It is used to avoid overfitting.\n",
    "An ensemble method is a technique which uses multiple independent similar or different models/weak\n",
    "learners to derive an output or make some predictions.\n",
    "\n",
    "Ensemble :\n",
    "1. Bagging (Parallel) : Bootstraping + aggrigating\n",
    "   RandomForest : It is collection mutiple DT (n_estimator = 100)\n",
    "2. Boosting (Sequential):\n",
    "   AdaBoost (Adaptive Boosting)\n",
    "   It is collection of stumps or It is collection forest of stumps (n_estimator = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abca5c",
   "metadata": {},
   "source": [
    "### 2. What is Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is sequential approach\n",
    "Boosting is a method of merging different types of predictions.\n",
    "Boosting decreases bias, not variance.\n",
    "In Boosting, models are weighed based on their performance.\n",
    "New models are affected by a previously built model’s performance in Boosting.\n",
    "Boosting is usually applied where the classifier is stable and simple and has high bias.\n",
    "In Boosting, every new subset comprises the elements that were misclassified previous models.\n",
    "\n",
    "AdaBoost (Adaptive Boosting)\n",
    "It is collection of stumps or It is collection forest of stumps (n_estimator = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efb761",
   "metadata": {},
   "source": [
    "### 3. Explain the difference between bagging and boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5670a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging:\n",
    "    Bagging is parallel approach\n",
    "    Bagging is a method of merging the same type of predictions.\n",
    "    Bagging decreases variance, not bias, and solves over-flttlng issues in a model.\n",
    "    In Bagging, each model receives an equal weight.\n",
    "    Models are built independently in Bagging.\n",
    "    Bagging is usually applied where the classifier is unstable and has a high variance\n",
    "\n",
    "Boosting:\n",
    "    Boosting is sequential approach\n",
    "    Boosting is a method of merging different types of predictions.\n",
    "    Boosting decreases bias, not variance.\n",
    "    In Boosting, models are weighed based on their performance.\n",
    "    New models are affected by a previously built model’s performance in Boosting.\n",
    "    Boosting is usually applied where the classifier is stable and simple and has high bias.\n",
    "    In Boosting, every new subset comprises the elements that were misclassified by previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d39c0",
   "metadata": {},
   "source": [
    "### 4. Explain the working of the AdaBoost Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Initialize weight (Sample Weight) : 1/ no of data points\n",
    "2) Best  Stumps\n",
    "     1) Use Gini >> lowest Gini\n",
    "3) Total Error : TE\n",
    "       TE = (Miss classified data points/ Total no of data points)\n",
    "4) Performance of model (p) = (1/2* loge(1-TE/TE)) = 0.9729550745276566\n",
    "5) New sample weight : S.W * e^p (miss clasification)\n",
    "                       S.W * e^-p ( Perfectly clasification)\n",
    "6) Normalize weight\n",
    "7) create buckets\n",
    "8) create random values between 0 to 1 >> random weight\n",
    "---\n",
    "1)Build the first WeakLearner: Find the stump with the best performance.\n",
    "2)Calculating the error of the stump.\n",
    "3)Calculate the weighting of the stump.\n",
    "4)Adjustment of the sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999d714",
   "metadata": {},
   "source": [
    "### 5. What are Weak Learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "A weak learner is a model that gives better results than a random prediction in a classification\n",
    "problem or the mean in a regression problem.\n",
    "\n",
    "The weak learners in AdaBoost are decision trees with a single split, called decision stumps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028f5fe",
   "metadata": {},
   "source": [
    "### 6. What is the difference between a Weak Learner vs a Strong Learner and why they could be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weak Learner:\n",
    "Weak learners are models that perform slightly better than random guessing.\n",
    "In boosting we use weak learners mostly since they are trained faster compared to strong learners.\n",
    "weak classifier is recommended to avoid overfitting and better control of the model complexity.\n",
    "AdaBoost works by putting more weight on difficult to classify instances and less on those already\n",
    "handled well.\n",
    "\n",
    "Strong Learner:\n",
    "Strong learners are models that have arbitrarily good accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28224565",
   "metadata": {},
   "source": [
    "### 7. What are the Stumps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddef8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "A decision stump is a machine learning model consisting of a one-level decision tree.\n",
    "The weak learners in AdaBoost are decision trees with a single split, called decision stumps.\n",
    "A decision stump is a machine learning model consisting of a one-level decision tree.\n",
    "That is, it is a decision tree with one internal node (the root) which is immediately connected to the\n",
    "terminal nodes (its leaves).\n",
    "A decision stump makes a prediction based on the value of just a single input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc939d",
   "metadata": {},
   "source": [
    "### 8. How to calculate Total Error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba3180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total Error is equal to the sum of the weights of the incorrectly classified samples.\n",
    "Total Error :(TE)\n",
    "TE = (Miss classified data points/ Total no of data points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e07505",
   "metadata": {},
   "source": [
    "### 9. How to calculate the Performance of the Stump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "If the total error is 0.5, then the performance of the stump will be zero.\n",
    "If the total error is 0 or 1, then the performance will become infinity or -infinity respectively.\n",
    "Best Stumps:\n",
    "1) Use Gini >> lowest Gini value will be stump for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee52dc",
   "metadata": {},
   "source": [
    "### 10. How to calculate the New Sample Weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The formula used for this is 'W=1/N' where N is the number of records.\n",
    "In this dataset, there are only 5 records, so the sample weight becomes 1/5 initially.\n",
    "Every record gets the same weight.\n",
    "New sample weight : S.W * e^p (miss clasification)\n",
    "S.W * e^-p ( Perfectly clasification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33fcda",
   "metadata": {},
   "source": [
    "### 11. How to create a New Dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Assign sample weight to records.\n",
    "2. Creating the stump in the forest.\n",
    "3. Calculate the Performance \n",
    "4. Update the sample weights.\n",
    "5. Normalise the weights.\n",
    "6. Creation of new dataset as per bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378301c",
   "metadata": {},
   "source": [
    "### 12. How Does the Algorithm Decide Output for Test Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a2e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the AdaBoost algorithm too, the majority of votes take place between the stumps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15076d94",
   "metadata": {},
   "source": [
    "### 13. Whether feature scaling is required in AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost do not require scaling because their splitting is based on values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a69b8b",
   "metadata": {},
   "source": [
    "### 14. List down the hyper-parameters used to fine-tune the AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e166d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. base_estimator: The model to the ensemble.It is a weak learner used to train the model\n",
    "2. n_estimators: Number of models to be built.Number of weak learners to train in each iteration.\n",
    "   n_estimators:by default(50)\n",
    "3. learning_rate: shrinks the contribution of each classifier by this value.\n",
    "                  It contributes to the weights of weak learners.\n",
    "   learning_rate:np.arange(0,2,0.0)\n",
    "4. random_state: The random number used so that the same random numbers generated every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2e0f9",
   "metadata": {},
   "source": [
    "### 15. What is the importance of the learning_rate hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate: This determines the weight applied to each estimator in the boosting process.\n",
    "The default is 1.\n",
    "Think of the learning rate as the size of steps taken while learning. A higher learning rate means taking bigger steps, while a lower learning rate means taking smaller steps.\n",
    "Smaller values such as 0.05, 0.1 force the algorithm to train slower but with high-performance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279da40b",
   "metadata": {},
   "source": [
    "### 16. What are the advantages of the AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.It is easy to use as we do not have to do many hyperparameters tunning as compared to other\n",
    "  algorithms.\n",
    "2.Adaboost increases the accuracy of the weak machine learning models.\n",
    "3.Adaboost has immunity from overfitting of data as it runs each model in a sequence and has a \n",
    "  weight associated with them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686d6e8",
   "metadata": {},
   "source": [
    "### 17. What are the disadvantages of the AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. AdaBoost uses a progressively learning boosting technique. Hence high-quality data is needed\n",
    "   in examples of AdaBoost vs Random Forest.\n",
    "2. It is also very sensitive to outliers and noise in data requiring the elimination of these\n",
    "   factors before using the data.\n",
    "3. The main disadvantage of Adaboost is that it needs a quality dataset.\n",
    "   Noisy data and outliers have to be avoided before adopting an Adaboost algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1e247",
   "metadata": {},
   "source": [
    "### 18. What are the applications of the AdaBoost Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63889d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Object Detection\n",
    "Text Classification\n",
    "Medical Diagnosis\n",
    "Credit Scoring\n",
    "Stock Market Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999d007",
   "metadata": {},
   "source": [
    "### 19. Can you use AdaBoost for regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69327888",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost algorithms can be used for both classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32043638",
   "metadata": {},
   "source": [
    "### 20. How to evaluate AdaBoost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e788fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Build a model and make predictions.\n",
    "2. Assign higher weights to miss-classified points.\n",
    "3. Build next model.\n",
    "4. Repeat steps 3 and 4.\n",
    "5. Make a final model using the weighted average of individual models.\n",
    "---\n",
    "AdaBoost (Adaptive Boosting) is an algorithm used in machine learning for classification tasks. It combines multiple weak or\n",
    "simple classifiers (also called \"base learners\") to create a strong classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

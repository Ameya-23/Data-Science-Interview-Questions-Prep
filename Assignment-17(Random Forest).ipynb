{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32afefb2",
   "metadata": {},
   "source": [
    "### 1. What are Ensemble Methods in Machine Learning? What Are They and Why Use Them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble Technique:\n",
    "    1. Ensemble technique combines multiple machine learning models into single model.\n",
    "    2. To handle Overfitting ensemble method is used.\n",
    "    3. The aim is to increase performance of model.\n",
    "\n",
    "Ensemble Methods:\n",
    "    1.Bagging (Parallel Approach)\n",
    "    2.Boosting (Sequential Approach)\n",
    "    \n",
    "1. Bagging:\n",
    "    1.Bagging is parallel approach\n",
    "    2.In bagging we merge same type of predictions.\n",
    "    3.Bagging decreases variance, not bias and solve Overfitting issue in a model.\n",
    "    4.In bagging each model receives an equal weight.\n",
    "      meaning > Bagging -> BoostStrapping(unique) + Aggrating\n",
    "       Random Forest\n",
    "2. Boosting:\n",
    "    1.Boosting is Sequential Approach\n",
    "    2.Boosting is method of merging different types of predictions\n",
    "    3.Boosting decreases bias not variance\n",
    "    4.In boosting model are weighted based upon their performance.\n",
    "    5.Boosting is applied where the classifier is stable and has high bias\n",
    "     AdaBoost, Gradient Boost,XGBoost  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eeb82e",
   "metadata": {},
   "source": [
    "### 2. Explain the difference between bagging and boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging:\n",
    "    1.Bagging is parallel approach\n",
    "    2.Bagging is a method of merging the same type of predictions.\n",
    "    3.Bagging decreases variance, not bias, and solves over-flttlng issues in a model.\n",
    "    4.In Bagging, each model receives an equal weight.\n",
    "    5.Models are built independently in Bagging.\n",
    "    6.Bagging is usually applied where the classifier is unstable and has a high variance\n",
    "    7.In Bagging, training data subsets are drawn randomly with a replacement for the training dataset\n",
    "\n",
    "Boosting:\n",
    "    1.Boosting is sequential approach\n",
    "    2.Boosting is a method of merging different types of predictions.\n",
    "    3.Boosting decreases bias, not variance.\n",
    "    4.In Boosting, models are weighed based on their performance.\n",
    "    5.New models are affected by a previously built model’s performance in Boostlng.\n",
    "    6.Boosting is usually applied where the classifier is stable and simple and has high bias.\n",
    "    7.In Boosting, every new subset comprises the elements that were misclassified by previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed54ee85",
   "metadata": {},
   "source": [
    "### 3. Why is Random Forest Algorithm popular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38591f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.It can perform both regression and classification problem.\n",
    "2.It takes less training time as compared to other algorithms.\n",
    "3.It predicts output with high accuracy,even for the large dataset it runs efficiently.\n",
    "4.It can also maintain accuracy when a large proportion of data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabf1fe",
   "metadata": {},
   "source": [
    "### 4. Can Random Forest Algorithm be used both for Continuous and Categorical Target Variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e92703",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. We can use Random Forest Algorithm  for both Continuous as well as Categorical Target Variables.\n",
    "2. Random Forest Algorithm can handle the data set containing continuous variables, as in the case of\n",
    "   regression, and categorical variables, as in the case of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39801b25",
   "metadata": {},
   "source": [
    "### 5. What do you mean by Bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. It is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms.\n",
    "2. Bagging is usually applied where the classifier is unstable and has a high variance\n",
    "3. In Bagging, training data subsets are drawn randomly with a replacement for the training dataset\n",
    "4. Bagging avoids overfitting of data and is used for both regression and classification models.\n",
    "5. Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly\n",
    "   used to reduce variance within a noisy dataset.\n",
    "6. It is used in regression as well as classification problems.\n",
    "7. Bagging is nothing but BoostStrapping(unique data) + Aggrating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14a89e",
   "metadata": {},
   "source": [
    "### 6. Explain the working of the Random Forest Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2618a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Select random samples from a given data or training set.\n",
    "2. This algorithm will construct a decision tree for every training data.\n",
    "3. Voting will take place by averaging the decision tree.\n",
    "4. Finally, select the most voted prediction result as the final prediction result.\n",
    "---\n",
    "1. Create multiple BoostStrapplng data set n_estimator = 100 (Default)\n",
    "   100 BD will be created using row sampling method\n",
    "2.Training multiple DT model\n",
    "3.Output of all base model will get aggrigated for classification : Used voting classifier\n",
    "                                          For Regression : Used mean of all Base model o/p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe01ea",
   "metadata": {},
   "source": [
    "### 7. Why do we prefer a Forest (collection of Trees) rather than a single Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random forest algorithm avoids and prevents overfitting by using multiple trees.\n",
    "Decision trees require low computation, thus reducing time to implement and carrying low accuracy\n",
    "To reduce the variance of the model.\n",
    "This means that at each split of the tree the model considers only a small subset of features rather\n",
    "than all of the features of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74389172",
   "metadata": {},
   "source": [
    "### 8. What do you mean by Bootstrap Sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c01b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bootstrapping sample is different because one samples with replacement from the sample itself.\n",
    "Bootstrap means that instead of training on all the observations, each tree of RF is trained on a subset\n",
    "of the observations.\n",
    "The chosen subset is called the bag, and the remaining are called Out of Bag samples.\n",
    "Multiple trees are trained on different bags, and later the results from all the trees are aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec6b64",
   "metadata": {},
   "source": [
    "### 9. What does random refer to in ‘Random Forest’?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10059db",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Random Forest are so called because each tree in the forest is built by randomly selecting a sample of the data and it is based on the decision tree\n",
    "2.Random observations to grow each tree.\n",
    "3.Random variables selected for splitting at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372cf69",
   "metadata": {},
   "source": [
    "### 10. List down the features of Bagged Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b782fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reduces variance by averaging the ensembles results.\n",
    "The resulting model uses the entire feature space when considering node splits.\n",
    "Bagging trees allow the trees to grow without pruning.\n",
    "Reducing the tree-depth sizes and resulting in high variance but lower bias which can help improve predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210435cf",
   "metadata": {},
   "source": [
    "### 11. What are the Limitations of Bagging Trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "It uses the entire feature space when creating splits in the trees.\n",
    "Suppose some variables within the feature space are indicating certain predictions, there is a risk of having a forest of correlated trees, which actually increases bias and reduces variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa256c",
   "metadata": {},
   "source": [
    "### 12. Explain how the Random Forests give output for Classification, and Regression problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Create multiple BoostStrapping data set n_estimator = 100 (Default)\n",
    "100 BD will be created using row sampling method\n",
    "2)Training multiple DT model\n",
    "3)Output of all base model will get aggrigated for classification : Used voting classifier\n",
    "or\n",
    "For Regression : Used mean of all Base model o/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41832e4",
   "metadata": {},
   "source": [
    "### 13. Does Random Forest need Pruning? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c994a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "No pruning takes place in random forest i.e each tree is grown fully.\n",
    "a random forest tree is fully grown and unpruned, and so naturally the feature space is split\n",
    "into more and smaller regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b575098",
   "metadata": {},
   "source": [
    "### 14. List down the hyper-parameters used to fine-tune the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.n_estimators=100(default) : Number of trees\n",
    "2.criterion='gini'\n",
    "3.max_depth=None\n",
    "4.min_samples_split=2\n",
    "5.min_samples_leaf=1\n",
    "6.max_features='sqrt'\n",
    "7.bootstrap=True\n",
    "8.oob_score=False\n",
    "9.random_state=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23410c43",
   "metadata": {},
   "source": [
    "### 15. What is the importance of max_feature hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "This resembles the number of maximum features provided to each tree in a random forest.\n",
    "The maximum number of features Random Forest is allowed to try in individual tree.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2afc8",
   "metadata": {},
   "source": [
    "### 16. What are the advantages and disadvantages of the Random Forest Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages:\n",
    "    1.It can perform both regression and classification tasks.\n",
    "    2.A random forest produces good predictions that can be understood easily.\n",
    "    3.It can handle large datasets efficiently.\n",
    "    4.The random forest algorithm provides a higher level of accuracy in predicting outcomes over\n",
    "      the decision tree algorithm.\n",
    "    5.It reduces overfitting in decision trees and helps to improve the accuracy.\n",
    "\n",
    "Disadvantages:\n",
    "    1.increased accuracy requires more trees.\n",
    "    2.More trees slow down model.\n",
    "    3.Cant describe relationships within data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f762c3a",
   "metadata": {},
   "source": [
    "### 17. What are the applications are random forests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification and regression tasks.\n",
    "Feature selection to identify important variables.\n",
    "Anomaly detection to find outliers.\n",
    "Building recommender systems for personalized recommendations.\n",
    "Image and text classification.\n",
    "Time series analysis for predicting future values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26b6a5",
   "metadata": {},
   "source": [
    "### 18. What are the Out-of-Bag samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data points were chosen randomly and with replacement and the data points which fail to be a part of\n",
    "that particular sample are known as OUT-OF-BAG points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6ba91",
   "metadata": {},
   "source": [
    "### 19. What is Out-of-Bag Error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the \n",
    "   prediction error of random forests, boosted decision trees, and other machine learning models \n",
    "   utilizing bootstrap aggregating (bagging) Bagging uses subsampling with replacement to create \n",
    "   training samples for the model to learn from.\n",
    "2. An error estimate is made for cases that were not used when constructing the tree. \n",
    "  This is called an out-of-bag(OOB) error .\n",
    "3. The out-of-bag (OOB) error is a way of calculating the prediction error of machine learning \n",
    "   models that use bootstrap aggregation (bagging) and other, boosted decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a83b94",
   "metadata": {},
   "source": [
    "### 20. How would you improve the performance of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Lower the number of estimators. If you want to increase the accuracy of your model,increase the number of trees.\n",
    "   Specify the maximum number of features to be included at each node split. This depends very heavily on your dataset.\n",
    "2. To speed up the random forest use lower the number of estimators want to increase the accuracy of model, increase the number of trees\n",
    "   The Parameters tuning is the best way to improve the accuracy of the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

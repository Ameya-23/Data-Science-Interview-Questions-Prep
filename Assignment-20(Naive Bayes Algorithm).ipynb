{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e782ebd2",
   "metadata": {},
   "source": [
    "### 1. What is a Naïve Bayes Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. It is used for unstructured data.\n",
    "2. The Naïve Bayes classifier is a popular supervised machine learning algorithm used for \n",
    "   classification tasks such as text classification. \n",
    "3. It is based on the Bayes’ Theorem for calculating probabilities and conditional probabilities.\n",
    "4. You can use it for real-time and multi-class predictions, text classifications, spam filtering, \n",
    "    sentiment analysis, and a lot more.\n",
    "5. Naïve Bayes Classifier is High Dimensional Data:High dimension data\n",
    "    It means no. of rows must be less than no. of columns\n",
    "    e.g: Text data\n",
    "    Here, every word is considered as column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48a515",
   "metadata": {},
   "source": [
    "### 2. What are the different types of Naive Bayes classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104bbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Gaussian Naive Bayes : Numerical data ( it must follow gaussian distribution : kdpplot,qqplot)\n",
    "    - If data with continuous form and it is following gaussian distribution then we can use this algorithm\n",
    "    \n",
    "2. Multinomial Naive Bayes : It uses TF-IDF concept for converting text data to Numerical data\n",
    "    \n",
    "3. Bernoulli Naive Bayes: It uses Bag of words concept for converting text data to Numerical data\n",
    "    Independent data must be binary data and Target column can have two classes, three classes.\n",
    "    Here 1 Bag = 1 Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de334d82",
   "metadata": {},
   "source": [
    "### 3. Why Naive Bayes is called Naive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes classifier assumes features are independent of each other. Since that is rarely \n",
    "possible in real-life data, the classifier is called naive.\n",
    "Naive means Mutually independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd577c74",
   "metadata": {},
   "source": [
    "### 4. Can you choose a classifier based on the size of the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d52372",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.If the training set is small, high bias / low variance models (e.g. Naive Bayes) tend to \n",
    "  perform better because they are less likely to overfit.\n",
    "    Eg : Naive Bayes works best when the training set is large.\n",
    "2.If the training set is large, low bias / high variance models (e.g. Logistic Regression) tend \n",
    "  to perform better because they can reflect more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316f5bd",
   "metadata": {},
   "source": [
    "### 5. Explain Bayes Theorem in detail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7eb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory that allows us to update our beliefs\n",
    "about an event based on new evidence. It establishes a relationship between conditional \n",
    "probabilities, enabling us to calculate the probability of an event given prior knowledge.\n",
    "\n",
    "Mathematically, Bayes' theorem can be stated as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) represents the probability of event A occurring given that event B has occurred.\n",
    "P(B|A) is the probability of event B occurring given that event A has occurred.\n",
    "P(A) and P(B) are the probabilities of events A and B occurring independently of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104e2e9",
   "metadata": {},
   "source": [
    "### 6. What is the formula given by the Bayes theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c33d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes theorem is used to determine conditional probability. When two events A and B are independent, \n",
    "P(A|B) = P(A) and P(B|A) = P(B)\n",
    "Formula:\n",
    "P(A/B)=P(B/A).P(A)/P(B)\n",
    "Formula explination:\n",
    "P (A | B) is the probability of event A being true when event B is true. \n",
    "P (B | A) is the probability of event B being true when event A is true. \n",
    "P (A) is the probability of event A being true. \n",
    "P (B) is the probability of event B being true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1dccd0",
   "metadata": {},
   "source": [
    "### 7. What is posterior probability and prior probability in Naïve Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92dcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.Posterior Probability in Naïve Bayes\n",
    "-->\n",
    "1.A posterior probability, in Bayesian statistics, is the revised or updated probability of an \n",
    "  event occurring after taking into consideration new information.\n",
    "2.The posterior probability is calculated by updating the prior probability using Bayes' theorem.\n",
    "3.In statistical terms, the posterior probability is the probability of event A occurring given \n",
    "  that event B has occurred.\n",
    "    \n",
    "The formula to calculate a posterior probability of A occurring given that B occurred:\n",
    "    P(A|B)= P(A∩B)/P(B) = P(A)×P(B∣A)/P(B)\n",
    "where:\n",
    "    A,B=Events\n",
    "    P(B∣A)=The probability of B occurring given that A is true\n",
    "    P(A) and P(B)=The probabilities of A occurring and B occurring independently of each other\n",
    "\n",
    "The posterior probability is thus the resulting distribution, P(A|B).\n",
    "\n",
    "Q.Prior probability in Naïve Bayes\n",
    "-->\n",
    "1.Prior probability, in Bayesian statistics, is the probability of an event before new data is \n",
    "  collected. This is the best rational assessment of the probability of an outcome based on the \n",
    "  current knowledge before an experiment is performed.\n",
    "2.In statistical terms, the prior probability is the basis for posterior probabilities.\n",
    "\n",
    "P(A|B)= P(A∩B)/P(B) = P(A)×P(B∣A)/P(B)\n",
    "\n",
    "where:\n",
    "P(A) = the prior probability of A occurring\n",
    "P(A∣B)= the conditional probability of A given that B occurs\n",
    "P(B∣A) = the conditional probability of B given that A occurs\n",
    "P(B) = the probability of B occurring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7ba6e",
   "metadata": {},
   "source": [
    "### 8. Define likelihood and evidence in Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.Likelihood in Naive Bayes:\n",
    "-->   \n",
    "Likelihood refers to the probability of observing the data that has been observed assuming that\n",
    "the data came from a specific scenario.\n",
    "\n",
    "Q.Evidencein Naive Bayes\n",
    "-->\n",
    "Using Bayes theorem, we can find the probability of A happening, given that B has occurred. \n",
    "Here, B is the evidence and A is the hypothesis. \n",
    "The assumption made here is that the predictors/features are independent. \n",
    "That is presence of one particular feature does not affect the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd15df5",
   "metadata": {},
   "source": [
    "### 9. Define Bayes theorem in terms of prior, evidence, and likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b0c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Prior: The prior probability, denoted as P(A), represents our initial belief or knowledge about \n",
    "       the probability of event A occurring before any new evidence is considered. It is based on\n",
    "       our previous information or assumptions.\n",
    "\n",
    "Evidence: The evidence is the new information or observation that we acquire, typically denoted as\n",
    "          B. It could be a set of data, a specific condition, or an observation related to the \n",
    "          event of interest.\n",
    "\n",
    "Likelihood: The likelihood, represented as P(B|A), is the probability of observing the evidence B \n",
    "            given that the event A has occurred. It quantifies how well the evidence supports or \n",
    "            aligns with the occurrence of the event.\n",
    "    \n",
    "Bayes' Theorem combines these elements to calculate the posterior probability, which is the \n",
    "updated probability of event A occurring given the new evidence B. Mathematically, \n",
    "it is expressed as:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24ab8d",
   "metadata": {},
   "source": [
    "### 10. How does the Naive Bayes classifier work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d08885",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Gather a labeled training dataset with instances and their corresponding class labels.\n",
    "2.Calculate the prior probability of each class based on the frequency of occurrence in the \n",
    "  training data.\n",
    "3.For each feature, estimate the likelihood probability of that feature given each class.\n",
    "4.Given a new instance with features, calculate the posterior probability of each class using \n",
    "  Bayes' theorem by multiplying the prior probability with the product of the likelihood \n",
    "  probabilities for the observed features.\n",
    "5.Select the class with the highest posterior probability as the predicted class for the new \n",
    "  instance.\n",
    "6.Repeat the prediction process for new instances to classify them into one of the learned classes\n",
    "  based on the highest posterior probability.\n",
    "7.Naive Bayes is known for its simplicity, speed, and effectiveness in handling high-dimensional \n",
    "  data, but its performance may be limited by the assumption of feature independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ca896",
   "metadata": {},
   "source": [
    "### 11. While calculating the probability of a given situation, what error can we run into in Naïve Bayes and how can we solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zero Frequence Error\n",
    "\n",
    "Steps to resolve:\n",
    "Steps used to solve zero frequency problem\n",
    "P(xi/Y) = (Nc + alpha) / (n+m)\n",
    "\n",
    "Nc : no of instances where y=yes or y=No\n",
    "alpha > laplace paramter > aplha =1\n",
    "n : Total no of y value i.e yes or No in target column\n",
    "m : no of unique value present in that particular column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e83db",
   "metadata": {},
   "source": [
    "### 12. How would you use Naive Bayes classifier for categorical features? What if some features are numerical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d89137",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.For categorical features, you can directly calculate likelihood probabilities based on the \n",
    "frequency of occurrences. \n",
    "2.If some features are numerical, they need to be discretized into bins or transformed into \n",
    "categorical features. \n",
    "3.Discretization involves dividing the numerical range into intervals or bins,and then assigning \n",
    "  values to corresponding bins. \n",
    "4.The transformed numerical feature is treated as a categorical feature. \n",
    "5. The choice of discretization method and number of bins can impact classifier performance. \n",
    "6.After discretization, likelihood probabilities for categorical features can be calculated as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9a294",
   "metadata": {},
   "source": [
    "### 13. What's the difference between Generative Classifiers and Discriminative Classifiers? Name some examples of each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bcd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Discriminative models draw boundaries in the data space, while generative models try to model how \n",
    "data is placed throughout the space.\n",
    "2.A generative model explains how the data was generated, while a discriminative model focuses on \n",
    "predicting the labels of the data.\n",
    "3.Generative models have more impact on outliers than discriminative models.\n",
    "4.Discriminative models are computationally cheap as compared to generative models.\n",
    "\n",
    "Examples:\n",
    "    \n",
    "Generative classifiers\n",
    "- Naïve Bayes\n",
    "- Bayesian networks\n",
    "- Markov random fields\n",
    "- Hidden Markov Models (HMM)\n",
    "\n",
    "Discriminative Classifiers\n",
    "- Logistic regression\n",
    "- Scalar Vector Machine\n",
    "- Traditional neural networks\n",
    "- Nearest neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3789689",
   "metadata": {},
   "source": [
    "### 14. Is Naive Bayes a discriminative classifier or generative classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes is considered a generative classifier. \n",
    "Although Naive Bayes can be used for classification tasks, it also provides a generative\n",
    "model that estimates the underlying probability distribution of the features and class labels. \n",
    "By modeling the joint distribution, Naive Bayes can generate synthetic samples that\n",
    "follow the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3079e3",
   "metadata": {},
   "source": [
    "### 15. Whether Feature Scaling is required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59725246",
   "metadata": {},
   "outputs": [],
   "source": [
    "As the Naive Bayes classifier is not dependent on the distance. Still, the probability hence for \n",
    "that reason feature scaling is not required, i.e, Any algorithm which is not dependent on \n",
    "distance will not require feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa656e",
   "metadata": {},
   "source": [
    "### 16. Impact of outliers on NB Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3674c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes is highly impacted by outliers and completely robust in this case \n",
    "depending on the USE case we are working on. The reason is the NB classifier assigns the 0 \n",
    "probability for all the data instances it has not seen in the training set, which creates an \n",
    "issue during the prediction time, and the same goes with outliers also, as it would have been the \n",
    "same data that the classifier has not seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d9de9",
   "metadata": {},
   "source": [
    "### 17. What is the Bernoulli distribution in Naïve Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde97034",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Naïve Bayes, the Bernoulli distribution models binary features (0 or 1). \n",
    "It represents the probability of success (1) given a parameter p. \n",
    "In Naïve Bayes, it is used to estimate the likelihood probabilities of binary features for each \n",
    "class. \n",
    "It is commonly applied in tasks like text classification, where features represent the presence or\n",
    "absence of specific words in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b40150",
   "metadata": {},
   "source": [
    "### 18. What are the advantages of the Naive Bayes Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.As it works independently with each feature, we can use it with large datasets for making \n",
    "  generalized models.\n",
    "2.It has very much less sensitive to other features, i.e.; it is not much affected by other \n",
    "  components because of its Naive nature.\n",
    "3.It tends to work efficiently with both continuous and discrete types of datasets and is \n",
    "  well-versed in handling categorical features in data.\n",
    "4.When we have a dataset with very less training data, then we can call up the Naive Bayes \n",
    "  classifier in this scenario it outperforms other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b17a64",
   "metadata": {},
   "source": [
    "### 19. What are the disadvantages of the Naive Bayes Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5381de",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.As we say that there are always two sides to a coin, the advantage of naive Bayes can also be a\n",
    "  disadvantage at some stages. As it treats all the predictors independently, for that reason, we\n",
    "  are not able to use it in all real-world cases.\n",
    "2.This algorithm faces a very major problem named the “Zero Frequency problem,” in which it \n",
    "  assigns zero probabilities to all the categorical variables whose categories were not present \n",
    "  in the training dataset, which introduces a lot of bias in the model.\n",
    "3.As the features are highly correlated, it affects the model performance negatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fae309",
   "metadata": {},
   "source": [
    "### 20. What are the applications of Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Text Classification \n",
    "- Email Filtering \n",
    "- Recommendation Systems \n",
    "- Medical Diagnosis \n",
    "- Fraud Detection \n",
    "- Customer Segmentation \n",
    "- News Classification \n",
    "- Social Media Analysis\n",
    "- Document Filtering \n",
    "- Language Detection "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
